# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q0fvylcxKh-BzSV2g5v_RuOx4RJ0Jm2g
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Модель"""

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

!pip freeze | grep "numpy\|pandas\|lightgbm\|scikit-learn"

"""## Загрузка данных"""

train_df = pd.read_parquet("/content/drive/MyDrive/train_data.pqt")
test_df = pd.read_parquet("/content/drive/MyDrive/test_data.pqt")

train_df.head(3)

test_df.head(3)

"""Обозначение категориальных признаков"""

cat_cols = [
    "channel_code", "city", "city_type",
    "okved", "segment", "start_cluster",
    "index_city_code", "ogrn_month", "ogrn_year",
]

train_df[cat_cols] = train_df[cat_cols].astype("category")
test_df[cat_cols] = test_df[cat_cols].astype("category")

"""## Подготовка данных

Работа с пропусками
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder, Normalizer, MinMaxScaler, RobustScaler

threshold = len(train_df.columns) / 2.1
train_df = train_df.dropna(thresh=threshold)

numeric_cols = train_df.select_dtypes(include=np.number).columns
train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].mean())

cat_cols_missing = [col for col in cat_cols if train_df[col].isnull().any()]
if cat_cols_missing:
    imp_mode = SimpleImputer(strategy='most_frequent')
    train_df[cat_cols_missing] = imp_mode.fit_transform(train_df[cat_cols_missing])

"""Кодирование категориальных признаков (ECF)"""

enc = LabelEncoder()

le = {}
for col in cat_cols:
  le[col] = enc
  train_df[col] = le[col].fit_transform(train_df[col])

"""Создаем выборки для валидации и обучения"""

X = train_df.drop(["id", "date", "end_cluster"], axis=1)
y = train_df["end_cluster"]

x_train, x_val, y_train, y_val = train_test_split(X, y,
                                                  test_size=0.3,
                                                  random_state=42)

y_train

"""## Обучение модели

LGBM
"""

pipeline = Pipeline([
    ('scaler', MinMaxScaler()),
    ('model', LGBMClassifier(verbosity=-1, random_state=42, n_jobs=-1, learning_rate=0.0075, n_estimators=40))
])

"""## Подбор параметров (для разработки)"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'model__n_estimators': [50, 70, 90],
    # 'model__learning_rate': [0.01, 0.1, 0.5],
    'model__learning_rate': [0.15, 0.2]
    # 'model__learning_rate': [0.2]
}

# Обучаем модель с использованием GridSearchCV для выбора оптимальных параметров
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', return_train_score=True)
grid_search.fit(x_train, y_train)

# Оцениваем производительность модели на валидационном наборе данных
accuracy = grid_search.score(x_val, y_val)
print("Accuracy:", accuracy)

# Получаем лучшие параметры модели
best_params = grid_search.best_params_
print("Best parameters:", best_params)

import matplotlib.pyplot as plt

from sklearn.model_selection import learning_curve

# Создание кривой обучения
train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=3, scoring='accuracy', train_sizes=[0.25, 0.5, 1])

# Вычисление средних значений и стандартных отклонений
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Построение графика
plt.plot(train_sizes, train_mean, 'o-', color='b', label='Ошибка на обучающей выборке')
plt.plot(train_sizes, test_mean, 'o-', color='r', label='Ошибка на валидационной выборке')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')
plt.xlabel('Размер обучающей выборки')
plt.ylabel('Оценка точности')
plt.legend(loc='best')
plt.show()

"""## Название не придумал, но тыкнуть надо"""

pipeline.fit(x_train, y_train)

"""Зададим функцию для взвешенной метрики roc auc"""

def weighted_roc_auc(y_true, y_pred, labels, weights_dict):
    unnorm_weights = np.array([weights_dict[label] for label in labels])
    weights = unnorm_weights / unnorm_weights.sum()
    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,
                                    multi_class="ovr", average=None)
    return sum(weights * classes_roc_auc)

cluster_weights = pd.read_excel("/content/drive/MyDrive/cluster_weights.xlsx").set_index("cluster")
weights_dict = cluster_weights["unnorm_weight"].to_dict()

"""## Проверка работы модели"""

y_pred_proba = pipeline.predict_proba(x_val)
y_pred_proba.shape

weighted_roc_auc(y_val, y_pred_proba, pipeline.classes_, weights_dict)

"""MinMaxScaler 0.8767

RobustScaler 0.8765

StandartScaler 0.83

## Прогноз на тестовой выборке
"""

test_df.pivot(index="id", columns="date", values="start_cluster").head(3)

"""Для того, чтобы сделать прогноз на тестовой выборке, нужно заполнить стартовый кластер. </br>
В качестве базового подхода заполним все стартовые кластеры, самым популярным кластером.
"""

test_df["start_cluster"] = train_df["start_cluster"].mode()[0]
test_df["start_cluster"] = test_df["start_cluster"].astype("category")

sample_submission_df = pd.read_csv("/content/drive/MyDrive/sample_submission.csv")

sample_submission_df.shape

sample_submission_df.head()

"""Для тестовой выборки будем использовать только последний месяц"""

last_m_test_df = test_df[test_df["date"] == "month_6"]
last_m_test_df = last_m_test_df.drop(["id", "date"], axis=1)

"""Кодирование категориальных признаков (ECF)"""

enc = LabelEncoder()

le = {}
for col in cat_cols:
  le[col] = enc
  last_m_test_df[col] = le[col].fit_transform(last_m_test_df[col])

last_m_test_df.head()

test_pred_proba = pipeline.predict_proba(last_m_test_df)
test_pred_proba_df = pd.DataFrame(test_pred_proba, columns=pipeline.classes_)
sorted_classes = sorted(test_pred_proba_df.columns.to_list())
test_pred_proba_df = test_pred_proba_df[sorted_classes]

test_pred_proba_df.shape

test_pred_proba_df.head(2)

sample_submission_df[sorted_classes] = test_pred_proba_df
sample_submission_df.to_csv("npb.csv", index=False)

r = pd.read_csv('npb.csv')
r