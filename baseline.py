# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q0fvylcxKh-BzSV2g5v_RuOx4RJ0Jm2g

# Baseline
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

!pip freeze | grep "numpy\|pandas\|lightgbm\|scikit-learn"

"""## Загрузка данных"""

train_df = pd.read_parquet("train_data.pqt")
test_df = pd.read_parquet("test_data.pqt")

train_df = pd.read_parquet("/content/drive/MyDrive/train_data.pqt")
test_df = pd.read_parquet("/content/drive/MyDrive/test_data.pqt")

train_df.head(9)

test_df.head(9)

cat_cols = [
    "channel_code", "city", "city_type",
    "okved", "segment", "start_cluster",
    "index_city_code", "ogrn_month", "ogrn_year",
]

"""Обозначение категориальных признаков"""

train_df[cat_cols] = train_df[cat_cols].astype("category")
test_df[cat_cols] = test_df[cat_cols].astype("category")

"""Создаем выборки для валидации и обучения"""

X = train_df.drop(["id", "date", "end_cluster"], axis=1)
y = train_df["end_cluster"]

x_train, x_val, y_train, y_val = train_test_split(X, y,
                                                  test_size=0.25,
                                                  random_state=42)

X

"""## Обучение модели

В качестве базовой модели возьмем LGBM обучим на всех признаках
"""

from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV

# Создаем экземпляр LGBMClassifier
lgbm = LGBMClassifier()

param_grid = {
    'learning_rate': [0.025, 0.05, 0.1],
    'max_depth': [1, 3, 5],
    'n_estimators': [25, 50, 100]
}

# Используем GridSearchCV для перебора параметров
grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=3, scoring='roc_auc')

# Обучаем модель с различными комбинациями параметров
grid_search.fit(x_train, y_train)

# Выводим наилучшие параметры
print("Наилучшие параметры:", grid_search.best_params_)

# Выводим ROC AUC на тестовом наборе данных
print("ROC AUC на тестовом наборе данных:", grid_search.best_score_)

model = LGBMClassifier(verbosity=-1, random_state=42, n_jobs=-1)
model.fit(x_train, y_train)

import matplotlib.pyplot as plt

from sklearn.model_selection import learning_curve

# Создание кривой обучения
train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=3, scoring='accuracy', train_sizes=[0.1, 0.25, 0.5, 0.75, 1.0])

# Вычисление средних значений и стандартных отклонений
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Построение графика
plt.plot(train_sizes, train_mean, 'o-', color='b', label='Ошибка на обучающей выборке')
plt.plot(train_sizes, test_mean, 'o-', color='r', label='Ошибка на валидационной выборке')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')
plt.xlabel('Размер обучающей выборки')
plt.ylabel('Оценка точности')
plt.legend(loc='best')
plt.show()

"""Зададим функцию для взвешенной метрики roc auc"""

def weighted_roc_auc(y_true, y_pred, labels, weights_dict):
    unnorm_weights = np.array([weights_dict[label] for label in labels])
    weights = unnorm_weights / unnorm_weights.sum()
    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,
                                    multi_class="ovr", average=None)
    return sum(weights * classes_roc_auc)

cluster_weights = pd.read_excel("cluster_weights.xlsx").set_index("cluster")
weights_dict = cluster_weights["unnorm_weight"].to_dict()

"""Проверка работы модели"""

y_pred_proba = model.predict_proba(x_val)
y_pred_proba.shape

weighted_roc_auc(y_val, y_pred_proba, model.classes_, weights_dict)

"""## Прогноз на тестовой выборке"""

test_df.pivot(index="id", columns="date", values="start_cluster").head(3)

"""Для того, чтобы сделать прогноз на тестовой выборке, нужно заполнить стартовый кластер. </br>
В качестве базового подхода заполним все стартовые кластеры, самым популярным кластером.
"""

test_df["start_cluster"] = train_df["start_cluster"].mode()[0]
test_df["start_cluster"] = test_df["start_cluster"].astype("category")

sample_submission_df = pd.read_csv("/content/drive/MyDrive/sample_submission.csv")

sample_submission_df = pd.read_csv("sample_submission.csv")

sample_submission_df.shape

sample_submission_df.head()

"""Для тестовой выборки будем использовать только последний месяц"""

last_m_test_df = test_df[test_df["date"] == "month_6"]
last_m_test_df = last_m_test_df.drop(["id", "date"], axis=1)

test_pred_proba = model.predict_proba(last_m_test_df)
test_pred_proba_df = pd.DataFrame(test_pred_proba, columns=model.classes_)
sorted_classes = sorted(test_pred_proba_df.columns.to_list())
test_pred_proba_df = test_pred_proba_df[sorted_classes]

test_pred_proba_df.shape

test_pred_proba_df.head(2)

sample_submission_df[sorted_classes] = test_pred_proba_df
sample_submission_df.to_csv("baseline_submission.csv", index=False)

r = pd.read_csv('baseline_submission.csv')
r

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

# Load data
train_df = pd.read_parquet("/content/drive/MyDrive/train_data.pqt")
test_df = pd.read_parquet("/content/drive/MyDrive/test_data.pqt")

# Define categorical columns
cat_cols = [
    "channel_code", "city", "city_type",
    "okved", "segment", "start_cluster",
    "index_city_code", "ogrn_month", "ogrn_year",
]

# Convert categorical columns to category dtype
train_df[cat_cols] = train_df[cat_cols].astype("category")
test_df[cat_cols] = test_df[cat_cols].astype("category")

# Prepare features and target
X = train_df.drop(["id", "date", "end_cluster"], axis=1)
y = train_df["end_cluster"]

# Split data into train and validation sets
x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# One-hot encode categorical variables
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
x_train_encoded = encoder.fit_transform(x_train[cat_cols])
x_val_encoded = encoder.transform(x_val[cat_cols])

# Scale numerical features
num_cols = [col for col in X.columns if col not in cat_cols]
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train[num_cols])
x_val_scaled = scaler.transform(x_val[num_cols])

# Combine encoded categorical features and scaled numerical features
x_train_final = np.concatenate([x_train_encoded, x_train_scaled], axis=1)
x_val_final = np.concatenate([x_val_encoded, x_val_scaled], axis=1)

# Load cluster weights
cluster_weights = pd.read_excel("/content/drive/MyDrive/cluster_weights.xlsx").set_index("cluster")
weights_dict = cluster_weights["unnorm_weight"].to_dict()

# Oversampling for imbalanced data
oversampler = RandomOverSampler(random_state=42)
x_train_resampled, y_train_resampled = oversampler.fit_resample(x_train_scaled, y_train)

# Define LightGBM model
model = LGBMClassifier(verbosity=-1, random_state=42, n_jobs=-1)

# Train the model
model.fit(x_train_resampled, y_train_resampled)

# Custom weighted ROC AUC function
def weighted_roc_auc(y_true, y_pred_proba, labels, weights_dict):
    unnorm_weights = np.array([weights_dict[label] for label in labels])
    weights = unnorm_weights / unnorm_weights.sum()
    classes_roc_auc = roc_auc_score(y_true, y_pred_proba, labels=labels,
                                    multi_class="ovr", average=None)
    return np.sum(weights * classes_roc_auc)

# Load cluster weights
cluster_weights = pd.read_excel("/content/drive/MyDrive/cluster_weights.xlsx").set_index("cluster")
weights_dict = cluster_weights["unnorm_weight"].to_dict()

# Make predictions on validation set
y_pred_proba = model.predict_proba(x_val_scaled)

# Compute weighted ROC AUC score
weighted_auc = weighted_roc_auc(y_val, y_pred_proba, model.classes_, weights_dict)
print("Weighted ROC AUC:", weighted_auc)

# Preprocess test data
test_features = test_df.drop(["id", "date"], axis=1)
test_features_scaled = scaler.transform(test_features)

# Make predictions on test data
test_df["end_cluster"] = model.predict(test_features_scaled)

# Save predictions to submission file
submission_df = test_df[["id", "end_cluster"]]
submission_df.to_csv("submission.csv", index=False)